• Bachelor’s degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative field.
• Minimum two years’ experience in a data engineer role or similar.
• Experience with cloud data warehousing like snowflake.
• Advanced working SQL knowledge and experience working with relational databases, SQL as well as working familiarity with a variety of databases.
• Experience building and optimizing ‘big data’ data pipelines, architectures and data sets: i.e. Hadoop, Spark, Kafka, etc.
• Experience with relational SQL and NoSQL databases, including Postgres.
• Experience with data pipeline and workflow management tools: i.e. Azkaban, Luigi, Airflow, etc.
• Experience with object-oriented/object function scripting languages: i.e. Python, Java, etc.
• Strong analytic skills related to working with unstructured datasets.
• Strong project management and organizational skills.
• Experience supporting and working with cross-functional teams in a dynamic environment.
• High-level interpersonal and cross-cultural skills.
• Must be a self-starter and highly organized.
• Analytical, creative thinking and problem solving skills.
• Flexibility and adaptability.
• Open to change.
• Calm under chaos.








What is a Data Pipeline:
Taking it from point A to B ---> Data cleansing
                                 Data Governance
                                 Data Enrichment
                                 Data Processing   
Data p[roducer - - - - - -  - Data Consumer

Why you need it? based on your requirements. For everything you need good quality data
So is it for: Data science, machine learning, business analytrics, reporting

Data pipeline vs ETL(extraaction, transform, l) - etl is a subset in a data pipeline mechanism
Suppose you want to process a daily sales data from a POS from a retail outlet? What to do? you will set up a batch process which will take the data at midnight from the retail stores, fetch the data and store it in the database and run the reports. Batch nodes are run daily - this is an ETL design. 
Suppose you want real tiume analysis such us understanding your product inventory, how you are replenishing the producst, how you are meeting the demands of the customers. Maybe every few minutes, you need real time data streaming, 

Under data pipeline you can have:
      -> Real time data pipeline
      -> Batch data pipeline
      -> Lambda architecture (real time & batch)          